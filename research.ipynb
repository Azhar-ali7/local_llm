{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the llm\n",
    "llm = Ollama(model = \"gemma:2b-instruct-q4_0\" , temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(\"Data/goog-10-k-2023 (1).pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "text_chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llmchatbot/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "knowledge_base = FAISS.from_documents(text_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrival chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    retriever = knowledge_base.as_retriever(),\n",
    "    llm = llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is this documentation?\"\n",
    "response = qa_chain.invoke({\"query\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context describes a table of contents for an annual report on Form 10-K for Alphabet Inc. It provides information about the company's compensation policies, including director and executive compensation.\n",
      "\n",
      "The information required by each item will be included in the corresponding captions in the table of contents.\n"
     ]
    }
   ],
   "source": [
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the company name here?\"\n",
    "response = qa_chain.invoke({\"query\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The company name in the context is Alphabet Inc.\n"
     ]
    }
   ],
   "source": [
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from io import BytesIO\n",
    "\n",
    "def save_uploadedfile(uploadedfile):\n",
    "    if not os.path.exists(\"tempDir\"):\n",
    "        os.makedirs(\"tempDir\")\n",
    "    file_path = os.path.join(\"tempDir\", uploadedfile.name)\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(uploadedfile.getbuffer())\n",
    "    return file_path\n",
    "\n",
    "def process_files(files):\n",
    "    documents = []\n",
    "    for file in files:\n",
    "        file_path = save_uploadedfile(file)\n",
    "        loader = UnstructuredFileLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "    return text_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llmchatbot/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/opt/anaconda3/envs/llmchatbot/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/anaconda3/envs/llmchatbot/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import FAISS, chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "def create_embeddings(documents):\n",
    "\n",
    "    # embeddings = OllamaEmbeddings(model=\"gemma:2b-instruct-q4_0\")\n",
    "    knowledge_base = FAISS.from_documents(documents, embeddings)\n",
    "    # knowledge_base = chroma.from_documents(documents, embeddings)\n",
    "\n",
    "    return knowledge_base\n",
    "\n",
    "def save_embeddings(knowledge_base, file_path):\n",
    "    # save to disk\n",
    "    # chroma.from_documents(knowledge_base, embeddings, persist_directory=file_path)\n",
    "    knowledge_base.save_local(file_path)\n",
    "\n",
    "def load_embeddings(file_path, ):\n",
    "    return FAISS.load_local(file_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    # return chroma(persist_directory=file_path, embedding_function=embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def create_qa_chain(knowledge_base):\n",
    "    llm = Ollama(model=\"gemma:2b-instruct-q4_0\", temperature=0.7)\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        retriever=knowledge_base.as_retriever(),\n",
    "        llm=llm\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "def ask_question(qa_chain, question):\n",
    "    response = qa_chain.invoke({\"query\": question})\n",
    "    return response['result']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the company name?\"\n",
    "\n",
    "loader = UnstructuredFileLoader(\"Data/goog-10-k-2023 (1).pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "text_split = split_documents(documents=documents)\n",
    "knowledge_base_1 = create_embeddings(text_split)\n",
    "qa_chain = create_qa_chain(knowledge_base_1)\n",
    "response = ask_question(qa_chain, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The company name is Alphabet Inc.\n",
      "\n",
      "The passage does not explicitly mention the company name, so I cannot answer this question from the provided context.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
